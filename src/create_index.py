# import json
# import uuid
# import logging
# from pathlib import Path
# from qdrant_client import QdrantClient
# from qdrant_client.http.models import Distance, VectorParams, PointStruct
# from sentence_transformers import SentenceTransformer
# from dotenv import load_dotenv
# import os
# from tqdm import tqdm

# # Load environment variables
# load_dotenv()

# # Qdrant Cloud configuration
# QDRANT_URL = os.getenv("QDRANT_URL")
# QDRANT_API_KEY = os.getenv("QDRANT_API_KEY")

# if not QDRANT_URL or not QDRANT_API_KEY:
#     raise ValueError("‚ö†Ô∏è QDRANT_URL ho·∫∑c QDRANT_API_KEY kh√¥ng ƒë∆∞·ª£c c·∫•u h√¨nh trong .env")

# # File paths
# CLEAN_CHUNKS_PATH = "D:/Vimedical/scripts/clean_chunks.json"
# QUESTIONS_PATH = "D:/Vimedical/scripts/questions_merged.json"

# # Collection names
# COLLECTION_QUESTIONS = "vimedical-questions"
# COLLECTION_INFORMATION = "vimedical-information"

# # Setup logging
# logging.basicConfig(level=logging.INFO)
# logger = logging.getLogger(__name__)

# # Initialize Qdrant Cloud Client
# qdrant_client = QdrantClient(
#     url=QDRANT_URL,
#     api_key=QDRANT_API_KEY,
#     timeout=120,
#     check_compatibility=False
# )

# # Load embedding model
# try:
#     model = SentenceTransformer("paraphrase-multilingual-MiniLM-L12-v2")
#     logger.info("‚úÖ M√¥ h√¨nh SentenceTransformer ƒë√£ ƒë∆∞·ª£c t·∫£i th√†nh c√¥ng!")
# except Exception as e:
#     logger.error(f"‚ùå L·ªói khi t·∫£i m√¥ h√¨nh SentenceTransformer: {e}")
#     raise

# def load_json_file(path):
#     try:
#         with open(path, "r", encoding="utf-8") as f:
#             data = json.load(f)
#             if not data:
#                 raise ValueError(f"‚ö†Ô∏è File {path} r·ªóng.")
#             return data
#     except Exception as e:
#         logger.error(f"‚ùå L·ªói khi t·∫£i file {path}: {e}")
#         raise

# def extract_chunks(data):
#     chunks = []
#     for item in data:
#         disease = item.get('title', '').split(':')[0].strip()
#         for section in item.get("sections", []):
#             text = section.get("content", "").strip()
#             if len(text.split()) > 10:
#                 chunks.append({
#                     "text": text,
#                     "metadata": {
#                         "disease": disease,
#                         "source": item.get("source", ""),
#                         "type": "information"
#                     }
#                 })
#     return chunks

# def extract_questions(data):
#     questions = []
#     for disease, qs in data.items():
#         for q in qs:
#             q_clean = q.strip()
#             if len(q_clean.split()) > 5:
#                 questions.append({
#                     "text": q_clean,
#                     "metadata": {
#                         "disease": disease,
#                         "source": "questions_merged",
#                         "type": "question"
#                     }
#                 })
#     return questions

# def create_collection_with_index(collection_name):
#     try:
#         collections = qdrant_client.get_collections().collections
#         if collection_name in [c.name for c in collections]:
#             logger.info(f"üßπ X√≥a collection c≈©: {collection_name}")
#             qdrant_client.delete_collection(collection_name)

#         qdrant_client.create_collection(
#             collection_name=collection_name,
#             vectors_config=VectorParams(size=384, distance=Distance.COSINE)
#         )
#         logger.info(f"‚úÖ Created collection {collection_name}")

#         # T·∫°o payload index cho tr∆∞·ªùng metadata.disease (s·ª≠ d·ª•ng API c≈©)
#         qdrant_client.create_payload_index(
#             collection_name=collection_name,
#             field_name="metadata.disease",
#             field_type="keyword"
#         )
#         logger.info(f"‚úÖ Created index on 'metadata.disease' in {collection_name}")

#     except Exception as e:
#         logger.error(f"‚ùå L·ªói khi t·∫°o collection {collection_name}: {e}")
#         raise

# def embed_and_upsert(texts, collection_name, batch_size=100):
#     if not texts:
#         logger.warning(f"‚ö†Ô∏è Kh√¥ng c√≥ d·ªØ li·ªáu ƒë·ªÉ upsert v√†o {collection_name}.")
#         return

#     try:
#         for i in tqdm(range(0, len(texts), batch_size), desc=f"Upserting {collection_name}"):
#             batch = texts[i:i + batch_size]
#             vectors = model.encode([item["text"] for item in batch], show_progress_bar=False)
#             points = [
#                 PointStruct(
#                     id=str(uuid.uuid4()),
#                     vector=vec.tolist(),
#                     payload={
#                         "text": item["text"],
#                         "metadata": item["metadata"]
#                     }
#                 ) for item, vec in zip(batch, vectors)
#             ]
#             qdrant_client.upsert(collection_name=collection_name, points=points)
#             logger.info(f"‚úÖ Uploaded {len(points)} points to {collection_name}")
#     except Exception as e:
#         logger.error(f"‚ùå L·ªói khi upsert v√†o collection {collection_name}: {e}")
#         raise

# def main():
#     try:
#         chunks_data = load_json_file(CLEAN_CHUNKS_PATH)
#         questions_data = load_json_file(QUESTIONS_PATH)

#         chunks = extract_chunks(chunks_data)
#         questions = extract_questions(questions_data)

#         create_collection_with_index(COLLECTION_INFORMATION)
#         create_collection_with_index(COLLECTION_QUESTIONS)

#         embed_and_upsert(chunks, COLLECTION_INFORMATION)
#         embed_and_upsert(questions, COLLECTION_QUESTIONS)

#         logger.info(f"‚úÖ ƒê√£ x·ª≠ l√Ω t·ªïng c·ªông {len(chunks)} th√¥ng tin v√† {len(questions)} c√¢u h·ªèi.")
#     except Exception as e:
#         logger.error(f"‚ùå L·ªói trong qu√° tr√¨nh ch√≠nh: {e}")
#         raise

# if __name__ == "__main__":
#     main()
import json
import uuid
import logging
from qdrant_client import QdrantClient
from qdrant_client.http.models import Distance, VectorParams, PointStruct
from sentence_transformers import SentenceTransformer
from dotenv import load_dotenv
import os
from tqdm import tqdm

# Load environment variables
load_dotenv()

# Qdrant Cloud configuration
QDRANT_URL = os.getenv("QDRANT_URL")
QDRANT_API_KEY = os.getenv("QDRANT_API_KEY")

if not QDRANT_URL or not QDRANT_API_KEY:
    raise ValueError("‚ö†Ô∏è QDRANT_URL ho·∫∑c QDRANT_API_KEY kh√¥ng ƒë∆∞·ª£c c·∫•u h√¨nh trong .env")

# File paths
CLEAN_CHUNKS_PATH = "D:/Vimedical/scripts/clean_chunks.json"
QUESTIONS_PATH = "D:/Vimedical/scripts/questions_merged.json"

# Collection names
COLLECTION_QUESTIONS = "vimedical-questions"
COLLECTION_INFORMATION = "vimedical-information"

# Setup logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# Initialize Qdrant Cloud Client
qdrant_client = QdrantClient(
    url=QDRANT_URL,
    api_key=QDRANT_API_KEY,
    timeout=120,
    check_compatibility=False
)

# Load embedding model
try:
    model = SentenceTransformer("paraphrase-multilingual-MiniLM-L12-v2")
    logger.info("‚úÖ M√¥ h√¨nh SentenceTransformer ƒë√£ ƒë∆∞·ª£c t·∫£i th√†nh c√¥ng!")
except Exception as e:
    logger.error(f"‚ùå L·ªói khi t·∫£i m√¥ h√¨nh SentenceTransformer: {e}")
    raise


# H√†m chu·∫©n h√≥a v√† s·ª≠a l·ªói ch√≠nh t·∫£ t√™n b·ªánh
def normalize_disease_name(disease_name):
    # Lo·∫°i b·ªè d·∫•u c√°ch th·ª´a
    normalized = disease_name.strip().replace("  ", " ")
    
    
    # Vi·∫øt hoa ch·ªØ ƒë·∫ßu m·ªói t·ª´
    normalized = " ".join(word.capitalize() for word in normalized.split())
    return normalized

def load_json_file(path):
    try:
        with open(path, "r", encoding="utf-8") as f:
            data = json.load(f)
            if not data:
                raise ValueError(f"‚ö†Ô∏è File {path} r·ªóng.")
            return data
    except Exception as e:
        logger.error(f"‚ùå L·ªói khi t·∫£i file {path}: {e}")
        raise

def extract_chunks(data):
    chunks = []
    for item in data:
        disease = normalize_disease_name(item.get('title', '').split(':')[0].strip())
        for section in item.get("sections", []):
            text = section.get("content", "").strip()
            if len(text.split()) > 10:
                chunks.append({
                    "text": text,
                    "metadata": {
                        "disease": disease,
                        "source": item.get("source", ""),
                        "type": "information"
                    }
                })
    return chunks

def extract_questions(data):
    questions = []
    for disease, qs in data.items():
        disease = normalize_disease_name(disease)
        for q in qs:
            q_clean = q.strip()
            if len(q_clean.split()) > 5:
                questions.append({
                    "text": q_clean,
                    "metadata": {
                        "disease": disease,
                        "source": "questions_merged",
                        "type": "question"
                    }
                })
    return questions

def create_collection_with_index(collection_name):
    try:
        collections = qdrant_client.get_collections().collections
        if collection_name in [c.name for c in collections]:
            logger.info(f"üßπ X√≥a collection c≈©: {collection_name}")
            qdrant_client.delete_collection(collection_name)

        qdrant_client.create_collection(
            collection_name=collection_name,
            vectors_config=VectorParams(size=384, distance=Distance.COSINE)
        )
        logger.info(f"‚úÖ Created collection {collection_name}")

        # T·∫°o payload index cho tr∆∞·ªùng metadata.disease
        qdrant_client.create_payload_index(
            collection_name=collection_name,
            field_name="metadata.disease",
            field_type="keyword"
        )
        logger.info(f"‚úÖ Created index on 'metadata.disease' in {collection_name}")

    except Exception as e:
        logger.error(f"‚ùå L·ªói khi t·∫°o collection {collection_name}: {e}")
        raise

def embed_and_upsert(texts, collection_name, batch_size=100):
    if not texts:
        logger.warning(f"‚ö†Ô∏è Kh√¥ng c√≥ d·ªØ li·ªáu ƒë·ªÉ upsert v√†o {collection_name}.")
        return

    try:
        for i in tqdm(range(0, len(texts), batch_size), desc=f"Upserting {collection_name}"):
            batch = texts[i:i + batch_size]
            vectors = model.encode([item["text"] for item in batch], show_progress_bar=False)
            points = [
                PointStruct(
                    id=str(uuid.uuid4()),
                    vector=vec.tolist(),
                    payload={
                        "text": item["text"],
                        "metadata": item["metadata"]
                    }
                ) for item, vec in zip(batch, vectors)
            ]
            qdrant_client.upsert(collection_name=collection_name, points=points)
            logger.info(f"‚úÖ Uploaded {len(points)} points to {collection_name}")
    except Exception as e:
        logger.error(f"‚ùå L·ªói khi upsert v√†o collection {collection_name}: {e}")
        raise

def main():
    try:
        chunks_data = load_json_file(CLEAN_CHUNKS_PATH)
        questions_data = load_json_file(QUESTIONS_PATH)

        chunks = extract_chunks(chunks_data)
        questions = extract_questions(questions_data)

        create_collection_with_index(COLLECTION_INFORMATION)
        create_collection_with_index(COLLECTION_QUESTIONS)

        embed_and_upsert(chunks, COLLECTION_INFORMATION)
        embed_and_upsert(questions, COLLECTION_QUESTIONS)

        logger.info(f"‚úÖ ƒê√£ x·ª≠ l√Ω t·ªïng c·ªông {len(chunks)} th√¥ng tin v√† {len(questions)} c√¢u h·ªèi.")
    except Exception as e:
        logger.error(f"‚ùå L·ªói trong qu√° tr√¨nh ch√≠nh: {e}")
        raise

if __name__ == "__main__":
    main()