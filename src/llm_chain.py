# from langchain_community.chat_models import ChatOpenAI
# from langchain.prompts import PromptTemplate
# import os
# from dotenv import load_dotenv
# from rag_chain import get_qa_chain
# import logging

# # Load environment variables
# load_dotenv()

# # Setup logging
# logging.basicConfig(level=logging.INFO)
# logger = logging.getLogger(__name__)

# # Táº£i mÃ´ hÃ¬nh LLM há»— trá»£ tiáº¿ng Viá»‡t
# def load_llm():
#     """Táº£i LLM tá»« OpenRouter"""
#     return ChatOpenAI(
#         model="deepseek/deepseek-r1-distill-llama-70b:free",
#         openai_api_key=os.getenv("OPENROUTER_API_KEY"),
#         base_url="https://openrouter.ai/api/v1",
#         temperature=0.5
#     )

# # Prompt tÃ¹y chá»‰nh Ä‘á»ƒ tráº£ lá»i báº±ng tiáº¿ng Viá»‡t
# def custom_prompt():
#     prompt = """
# Báº¡n lÃ  má»™t trá»£ lÃ½ y táº¿ thÃ´ng minh. HÃ£y tráº£ lá»i cÃ¢u há»i cá»§a ngÆ°á»i dÃ¹ng dá»±a trÃªn thÃ´ng tin bÃªn dÆ°á»›i.
# Äá»«ng cá»‘ gáº¯ng Ä‘Æ°a ra cÃ¢u tráº£ lá»i náº¿u khÃ´ng cÃ³ thÃ´ng tin liÃªn quan trong tÃ i liá»‡u y táº¿ (context).
# Náº¿u khÃ´ng biáº¿t cÃ¢u tráº£ lá»i, hÃ£y tráº£ lá»i nhÆ° sau: "Xin lá»—i, tÃ´i khÃ´ng tÃ¬m tháº¥y thÃ´ng tin liÃªn quan trong tÃ i liá»‡u y táº¿ hiá»‡n cÃ³."

# ThÃ´ng tin y táº¿:
# {context}

# CÃ¢u há»i: {question}

# HÃ£y tráº£ lá»i báº±ng tiáº¿ng Viá»‡t, ngáº¯n gá»n, rÃµ rÃ ng. Náº¿u cÃ³ thá»ƒ, hÃ£y Ä‘á» cáº­p Ä‘áº¿n tÃªn bá»‡nh lÃ½ hoáº·c triá»‡u chá»©ng liÃªn quan.
# """
#     return PromptTemplate(template=prompt, input_variables=["context", "question"])

# # HÃ m kiá»ƒm tra xem cÃ¢u há»i cÃ³ yÃªu cáº§u thÃ´ng tin thÃªm hay khÃ´ng
# def is_requesting_more_info(query):
#     more_info_keywords = [
#         "thÃ´ng tin thÃªm", "chi tiáº¿t", "nÃ³i thÃªm", "giáº£i thÃ­ch", "tÃ¬m hiá»ƒu thÃªm",
#         "thÃªm thÃ´ng tin", "hiá»ƒu thÃªm", "thÃ´ng tin chi tiáº¿t", "bá»‡nh nÃ y", "vá» bá»‡nh nÃ y"
#     ]
#     query_lower = query.lower()
#     return any(keyword in query_lower for keyword in more_info_keywords)

# # Táº¡o LLM Chain
# def get_llm_chain():
#     llm = load_llm()
#     prompt = custom_prompt()
#     # Khá»Ÿi táº¡o rag_chain vá»›i bá»™ nhá»›
#     memory = {"last_disease": None}
#     rag_chain = get_qa_chain(memory=memory)

#     def run(query):
#         try:
#             # XÃ¡c Ä‘á»‹nh loáº¡i cÃ¢u há»i
#             if is_requesting_more_info(query):
#                 # Náº¿u yÃªu cáº§u thÃ´ng tin thÃªm, sá»­ dá»¥ng tÃªn bá»‡nh tá»« bá»™ nhá»›
#                 logger.info("ğŸ” YÃªu cáº§u thÃ´ng tin thÃªm, sá»­ dá»¥ng tÃªn bá»‡nh tá»« bá»™ nhá»›...")
#                 response = rag_chain(query, use_memory=True)
#             else:
#                 # Náº¿u khÃ´ng, thá»±c hiá»‡n truy váº¥n bÃ¬nh thÆ°á»ng (bÆ°á»›c 1: Ä‘oÃ¡n bá»‡nh)
#                 logger.info("ğŸ” Xá»­ lÃ½ cÃ¢u há»i triá»‡u chá»©ng...")
#                 response = rag_chain(query, use_memory=False)

#             # Xá»­ lÃ½ pháº£n há»“i tá»« rag_chain
#             if "result" in response:
#                 return {
#                     "context": response.get("context", ""),
#                     "disease": response.get("disease", ""),
#                     "result": response["result"],
#                     "source_documents": response.get("source_documents", [])
#                 }

#             # Láº¥y context vÃ  disease tá»« response
#             context = response.get("context", "")
#             disease = response.get("disease", "bá»‡nh khÃ´ng xÃ¡c Ä‘á»‹nh")
#             source_docs = response.get("source_documents", [])

#             # Chuáº©n bá»‹ cÃ¢u tráº£ lá»i tá»« LLM náº¿u cÃ³ context
#             if context:
#                 prompt_input = prompt.format(context=context, question=query)
#                 answer = llm.invoke(prompt_input).content
#             else:
#                 # Náº¿u khÃ´ng cÃ³ context, táº¡o cÃ¢u tráº£ lá»i cÆ¡ báº£n tá»« disease
#                 answer = f"TÃ´i khÃ´ng tÃ¬m tháº¥y thÃ´ng tin chi tiáº¿t vá» {disease} trong tÃ i liá»‡u y táº¿ hiá»‡n cÃ³. Tuy nhiÃªn, dá»±a trÃªn triá»‡u chá»©ng báº¡n mÃ´ táº£, báº¡n cÃ³ thá»ƒ Ä‘ang gáº·p pháº£i {disease}. HÃ£y Ä‘áº¿n khÃ¡m bÃ¡c sÄ© Ä‘á»ƒ Ä‘Æ°á»£c cháº©n Ä‘oÃ¡n chÃ­nh xÃ¡c."

#             # Ghi log xá»­ lÃ½
#             logger.info(f"ğŸ” Context: {context[:50]}... | Disease: {disease} | Answer: {answer}")

#             # Tráº£ vá» dá»¯ liá»‡u cho agent.py xá»­ lÃ½
#             return {
#                 "context": context,
#                 "disease": disease,
#                 "result": answer,
#                 "source_documents": source_docs
#             }

#         except Exception as e:
#             logger.error(f"âŒ Lá»—i trong LLM chain: {e}")
#             return {
#                 "context": "",
#                 "disease": "",
#                 "result": f"ÄÃ£ xáº£y ra lá»—i: {str(e)}",
#                 "source_documents": []
#             }

#     return run 

from langchain_community.chat_models import ChatOpenAI
from langchain.prompts import PromptTemplate
import os
from dotenv import load_dotenv
from rag_chain import get_qa_chain
import logging

# Load environment variables
load_dotenv()

# Setup logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# Táº£i mÃ´ hÃ¬nh LLM há»— trá»£ tiáº¿ng Viá»‡t
def load_llm():
    """Táº£i LLM tá»« OpenRouter"""
    return ChatOpenAI(
        model="deepseek/deepseek-r1-distill-llama-70b:free",
        openai_api_key=os.getenv("OPENROUTER_API_KEY"),
        base_url="https://openrouter.ai/api/v1",
        temperature=0.5
    )

# Prompt tÃ¹y chá»‰nh Ä‘á»ƒ tráº£ lá»i báº±ng tiáº¿ng Viá»‡t
def custom_prompt():
    prompt = """
Báº¡n lÃ  má»™t trá»£ lÃ½ y táº¿ thÃ´ng minh. HÃ£y tráº£ lá»i cÃ¢u há»i cá»§a ngÆ°á»i dÃ¹ng dá»±a trÃªn thÃ´ng tin bÃªn dÆ°á»›i.
Äá»«ng cá»‘ gáº¯ng Ä‘Æ°a ra cÃ¢u tráº£ lá»i náº¿u khÃ´ng cÃ³ thÃ´ng tin liÃªn quan trong tÃ i liá»‡u y táº¿ (context).
Náº¿u khÃ´ng biáº¿t cÃ¢u tráº£ lá»i, hÃ£y tráº£ lá»i nhÆ° sau: "Xin lá»—i, tÃ´i khÃ´ng tÃ¬m tháº¥y thÃ´ng tin liÃªn quan trong tÃ i liá»‡u y táº¿ hiá»‡n cÃ³."

ThÃ´ng tin y táº¿:
{context}

CÃ¢u há»i: {question}

HÃ£y tráº£ lá»i báº±ng tiáº¿ng Viá»‡t, ngáº¯n gá»n, rÃµ rÃ ng. Náº¿u cÃ³ thá»ƒ, hÃ£y Ä‘á» cáº­p Ä‘áº¿n tÃªn bá»‡nh lÃ½ hoáº·c triá»‡u chá»©ng liÃªn quan.
"""
    return PromptTemplate(template=prompt, input_variables=["context", "question"])

# HÃ m kiá»ƒm tra xem cÃ¢u há»i cÃ³ yÃªu cáº§u thÃ´ng tin thÃªm hay khÃ´ng
def is_requesting_more_info(query):
    more_info_keywords = [
        "thÃ´ng tin thÃªm", "chi tiáº¿t", "nÃ³i thÃªm", "giáº£i thÃ­ch", "tÃ¬m hiá»ƒu thÃªm",
        "thÃªm thÃ´ng tin", "hiá»ƒu thÃªm", "thÃ´ng tin chi tiáº¿t", "bá»‡nh nÃ y", "vá» bá»‡nh nÃ y"
    ]
    query_lower = query.lower()
    return any(keyword in query_lower for keyword in more_info_keywords)

# Táº¡o LLM Chain
def get_llm_chain():
    llm = load_llm()
    prompt = custom_prompt()
    # Khá»Ÿi táº¡o rag_chain vá»›i bá»™ nhá»›
    memory = {"last_disease": None}
    rag_chain = get_qa_chain(memory=memory)

    def run(query, last_disease=None):
        try:
            # Äá»“ng bá»™ last_disease tá»« interface náº¿u cÃ³
            if last_disease and last_disease != "bá»‡nh khÃ´ng xÃ¡c Ä‘á»‹nh":
                memory["last_disease"] = last_disease
                logger.info(f"ğŸ”„ ÄÃ£ Ä‘á»“ng bá»™ last_disease tá»« interface: {last_disease}")

            # XÃ¡c Ä‘á»‹nh loáº¡i cÃ¢u há»i
            if is_requesting_more_info(query):
                # Náº¿u yÃªu cáº§u thÃ´ng tin thÃªm, sá»­ dá»¥ng tÃªn bá»‡nh tá»« bá»™ nhá»›
                logger.info("ğŸ” YÃªu cáº§u thÃ´ng tin thÃªm, sá»­ dá»¥ng tÃªn bá»‡nh tá»« bá»™ nhá»›...")
                if not memory["last_disease"]:
                    return {
                        "context": "",
                        "disease": "",
                        "result": "Vui lÃ²ng cung cáº¥p tÃªn bá»‡nh hoáº·c há»i vá» triá»‡u chá»©ng trÆ°á»›c.",
                        "source_documents": []
                    }
                response = rag_chain(query, use_memory=True)
            else:
                # Náº¿u khÃ´ng, thá»±c hiá»‡n truy váº¥n bÃ¬nh thÆ°á»ng (bÆ°á»›c 1: Ä‘oÃ¡n bá»‡nh)
                logger.info("ğŸ” Xá»­ lÃ½ cÃ¢u há»i triá»‡u chá»©ng...")
                response = rag_chain(query, use_memory=False)

            # Xá»­ lÃ½ pháº£n há»“i tá»« rag_chain
            if "result" in response:
                return {
                    "context": response.get("context", ""),
                    "disease": response.get("disease", ""),
                    "result": response["result"],
                    "source_documents": response.get("source_documents", [])
                }

            # Láº¥y context vÃ  disease tá»« response
            context = response.get("context", "")
            disease = response.get("disease", "bá»‡nh khÃ´ng xÃ¡c Ä‘á»‹nh")
            source_docs = response.get("source_documents", [])

            # Chuáº©n bá»‹ cÃ¢u tráº£ lá»i tá»« LLM náº¿u cÃ³ context
            if context:
                prompt_input = prompt.format(context=context, question=query)
                answer = llm.invoke(prompt_input).content
            else:
                # Náº¿u khÃ´ng cÃ³ context, táº¡o cÃ¢u tráº£ lá»i cÆ¡ báº£n tá»« disease
                answer = f"TÃ´i khÃ´ng tÃ¬m tháº¥y thÃ´ng tin chi tiáº¿t vá» {disease} trong tÃ i liá»‡u y táº¿ hiá»‡n cÃ³. Tuy nhiÃªn, dá»±a trÃªn triá»‡u chá»©ng báº¡n mÃ´ táº£, báº¡n cÃ³ thá»ƒ Ä‘ang gáº·p pháº£i {disease}. HÃ£y Ä‘áº¿n khÃ¡m bÃ¡c sÄ© Ä‘á»ƒ Ä‘Æ°á»£c cháº©n Ä‘oÃ¡n chÃ­nh xÃ¡c."

            # Ghi log xá»­ lÃ½
            logger.info(f"ğŸ” Context: {context[:50]}... | Disease: {disease} | Answer: {answer}")

            # Tráº£ vá» dá»¯ liá»‡u
            return {
                "context": context,
                "disease": disease,
                "result": answer,
                "source_documents": source_docs
            }

        except Exception as e:
            logger.error(f"âŒ Lá»—i trong LLM chain: {e}")
            return {
                "context": "",
                "disease": "",
                "result": f"ÄÃ£ xáº£y ra lá»—i: {str(e)}",
                "source_documents": []
            }

    return run